\documentclass[11pt,nocut]{article}

\usepackage{../../latex_style/packages}
\usepackage{../../latex_style/notations}

\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Hints for the review exercises}
%\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{}
%\setcounter{section}{*}

\begin{document}
\maketitle
%\input{./preamble_homeworks.tex}
\begin{center}
	{\large
	hints. please only look at the hints if you have spent a reasonable time thinking about the problems!
	}
\end{center}
\section{2019 review exercises}

\begin{enumerate}
	\item Use the fact that $\|Ax\|^2 = x^{\sT} A^{\sT} A x$ and then use the SVD decomposition of $A$ to rewrite $A^{\sT} A$.
	\item Use the SVD of $A$.
	\item (a) True (b) False (c) False (eigenvalues can be negative but singular values can not. The singular values of a symmetric matrix are the absolute value of its eigenvalues).
	\item Use the definitions of kernel and image.
	\item Use the SVD decomposition of $A$ to compute $(A^{\sT}A)^{-1}A^{\sT}$ and see that it corresponds to the definition of $\A^{\dagger}$.
	\item Convex, convex, not convex.
	\item Convex, not convex, convex.
	\item Express the columns of $B$ using the left-singular vectors of the matrix $A$ whose rows are the $a_i$.
	\item False. False. True.
	\item Show that $(x+y)/2$ is a global minimizer of $f$.
	\item Normalizing the dataset is useless for ordinary least-squares, but can be useful for Lasso.
	\item Compute gradient and Hessian. 
	\item Use Lagrange multipliers.
	\item If the columns of $A$ are linearly dependent, then $f$ will be $L$-smooth but not strongly convex, hence the speed of gradient descent will be $O(1/t)$.
		If the columns of $A$ are linearly independent then you can show that $f(x)$ is $\mu$-strongly convex and $L$-smooth, for some $\mu,L>0$. Hence the error of gradient descent will be $O(e^{-\rho t})$ after $t$ steps, for some constant $\rho>0$. 
	\item 1 step.
	\item See lecture notes.
\end{enumerate}

\section{2018 review exercises}

\begin{enumerate}
	\item Show that for all $x \in \R^n$, $ABx=BAx$. (You can decompose such $x$ in the given basis)
	\item (a) See homework 10. (b) Use (a).
	\item Use the definition of eigenvectors/eigenvalues.
	\item Using the spectral Theorem there exists an orthonormal basis $(v_1, \dots, v_n)$ of $\R^n$ consisting of eigenvectors of $A$. Decompose $x$ in such a basis and compute $Ax$.
	\item If $f: \R^2 \to \R$ is convex, and if $(\alpha^*,\beta^*)$ is a minimizer of $f$ then $\nabla f(\alpha^*,\beta^*) = 0$.
	\item Use the definition of $\|x\|_{\infty}$ and $\|x\|$.
	\item By the spectral theorem, you can decompose $x$ in an orthonormal basis of $\R^n$ made of eigenvectors of $A$.
	\item Many possible ways to do this. (a) Show that $\Ker(A^{\sT}) = \Ker(AA^{\sT})$, and then use the rank-nullity theorem and the fact that $\rank(A) = \rank(A^{\sT})$. (b) Compute $AA^{\sT}$ using the SVD of $A$: $A = U \Sigma V^{\sT}$.
	\item (a) Use Lagrange multipliers. (b)  The set of solution of $Ax=b$ is $A^+ b + \Ker(A)$. The result follow from the same arguments than problem 1 of homework 10. 
	\item False.
	\item Show that $\|x+y\|^2 = \|x\|^2 + 2 \langle x,y \rangle + \|y\|^2$.
	\item Show that $\sum_{i=1}^n \langle x,u_i \rangle^2 = \|x\|^2$.
	\item Compute $AA^{\sT}$.
	\item Show that if $\lambda$ is an eigenvalue of $A$ associated with the eigenvector $u$ if and only if $Qu$ is an eigenvector of $B$ with eigenvalue $\lambda$.
	\item Justify that $x = \sum_{i=1}^m \langle x,v_i\rangle v_i$. Then expand $\| \sum \langle x,v_i \rangle v_i \|^2$ and make simplifications.
	\item Use the SVD of $A$.
	\item (a) See Homework 3. (b) Let $V \in \R^{n \times n}$ be the matrix whose columns are $v_1, \dots, v_n$. Show that $\Tr(V^T A v) = \sum_{i=1}^n v_i^{\sT} A v_i$. Then use (a). (c) Use the spectral theorem and (a).
	\item Use Problem 1.b from homework 7.
	\item Expand the right-hand side.
	\item (a). $A^2 = 0$. (b) Take for instance
		$$
		\begin{pmatrix}
			0 & 1 \\
			0 & 0
		\end{pmatrix}
		$$
	\item (a) convex, (b) not convex (c) not convex (d) convex. One can verify these points by computing the Hessian.
	\item (a) ... (b) There is a unique global minima.
	\item $V$ is of dimension $2$, hence $\dim(V^{\perp}) = 4-2 = 2$. $v_1 = (1,-1,0,0)$ and $v_2 = (0, 0, 1,-1)$ work.
	\item (a) Yes. (b) The derivative of a sum is equal to the sum of the derivaties, and the derivatives of $\lambda p$ (for some $\lamdba \in \R$) is equal to $\lambda p'$.
		(c) $\Ker(\mathcal{D})$ is the set of polynomials $p$ that are constant (i.e.\ there exists $a\in\R$ such that $p(x) =a$ for all $x \in \R$).
		(d) $\Im(\mathcal{D}) = \mathcal{P}_{d-1}$.
		(e) (i) check the usual conditions (ii) For polynomial of degree $\leq d$, Taylor formula of order $d$ is exact:
		$$
		T_s(p)(x) = p(x+s) = \sum_{k=0}^d \frac{p^{(k)}(x)}{k!}s^k = \sum_{k=0}^d \frac{\mathcal{D}^k(p)(x)}{k!}s^k.
		$$
		(iii) The matrix has $0$ below the diagonal and for $j \geq i$, $M_{i,j} = \binom{j-1}{i-1}$.
	\item (a) Let $B$ be a rank $1$ matrix. One can therefore write $B = u v^T$ for some $u \in \R^m$ and $v \in \R^n$.
		$$
		\|A-B\|_F^2 = \|A\|^2_F - 2 u^T A v + \|u\|^2 \|v\|^2.
		$$
		Now, $u^T Av \leq \|u\| \|v\| \sigma_1$. Hence, writing $r=\|u\|\|v\|$
		$$
		\|A-B\|_F^2 \geq \sum_{i=1}^{\min(n,m)} \sigma_i^2 - 2 \sigma_1 r + r^2
		= \sum_{i=2}^{\min(n,m)} \sigma_i^2 + (\sigma_1 -r)^2
		= \|A-A'\|_{F}^2 + (\sigma_1 - r)^2
		$$
		(b) Let $B = u v^T$ be a rank 1 matrix. Let $v_1, v_2$ be the first two right-singular vectors of $A$. $Span(v)^{\perp}$ has dimension $n-1$, hence one can find a vector of unit norm $z$ in $Span(v)^{\perp} \cap \Span(v_1,v_2)$. We write $z = \alpha_1 v_1 + \alpha_2 v_2$. Since $\|z\|=1$ and $v_1, v_2$ orthogonal, we have $\alpha_1^2 + \alpha_2^2 = 1$. By definition of the spectral norm
		$$
		\|A-B\|_{\rm Sp} \geq \|(A-B)z\| = \|Az\| = \sqrt{\alpha_1^2 \sigma_1^2 + \alpha_2^2 \sigma_2^2} \geq \sigma_2 =  \|A-A'\|_{\rm Sp}.
		$$
	\item $xx^T$ is rank $1$ and has two distinct eigenvalues $0$ and $1$. Hence $H$ has two distinct eigenvalues $-1$ and $1$.
	\item The vector $(1,1, \dots,1)$ is an eigenvector associated with the eigenvalue $d$. By contradiction let $x$ be an eigenvector associated with the eigenvalue $\lambda >d$. Let $i$ such that $|x_i| = \|x\|_{\infty} >0$. Then
		$$
		|x_i| \lambda 
		= |\sum_{j=1}^n G_{i,j} x_j|
		\leq \sum_{j=1}^n G_{i,j} |x_j|
		\leq |x_i| \sum_{j=1}^n G_{i,j} = d |x_i|.
		$$
		We get a contradiction.
	\item Use the matrix product formula.
	\item (a) convex but not subspace (b) not convex (c) subspace (hence convex)
	\item (a) convex but not strictly convex (b) convex but not strictly convex (c) convex but not strictly convex (d) not convex (e) not convex
	\item Cauchy-Schwarz.
	\item Apply the spectral theorem to $A$.
\end{enumerate}

\section{2018 final}

\begin{enumerate}
	\item (a) Show that $w \defeq u-v \neq 0$ belongs to $\Ker(A)$ and then that $x=u+tw$ is a solution to $Ax=b$ for all $t \in \R$. (b) Use the $w$ defined in (a).
	\item (a). Prove that $\rank(A^T A) = n$. Then use the fact that $\rank(A^T A) \leq \rank(A) \leq \min(n,m)$. (b) Show that $\rank(A) = n$, then use the rank-nullity theorem to get $\dim \Ker(A) = 0$. (c) Use the fact that for any $v \in \R^n$, $\|Av\|^2 = v^{\sT} A^T A v$.
	\item (a) $\rank(U) = n$. This comes from the fact that $\rank(U) \leq n$ (because $U$ is $n \times m$) and $\rank(U) \geq \rank(U^T U) \leq \rank(\Id_n) = n$. (b) Use the rank-nullity theorem.
		(c) Expand and simplify $\|y-Ux\|^2$.
		(d) Write $f(x) = \|y-Ux\|^2$ and solve $\nabla f(x) = 0$.
	\item (a) Convex set ($\Ker(A)$)
		(b) Convex set
		(c) Not always convex (take for instance for $A=\Id$)
		(d) Not convex.
		(e) Convex.
	\item (a)(e)(d)(f)
	\item (a) $f$ is convex (compute its Hessian).
		(b) $h$ is not convex (compute the Hessian)
		(c) Solve the equations $->$ Global minimum
		(d) Solve the equations $->$ Saddle point
	\item (a) By contradiction if for all $j$ we have $|v_i^T u_1| < \frac{1}{\sqrt{n}}$ then we get (since $v_1, \dots, v_n$ orthonormal basis)
		$$
		\|u_1\|^2 = \sum_{i=1}^n (v_i^T u_1)^2 < 1,
		$$
		which is a contradiction.
		(b) Take $u_1=(1,0)$, $u_2=(0,1)$, $v_1 = (1/\sqrt{2}, 1/\sqrt{2})$, $v_2 = (1/\sqrt{2},-1/\sqrt{2})$.
\end{enumerate}

\vspace{1cm}
\centerline{\pgfornament[width=7cm]{87}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
