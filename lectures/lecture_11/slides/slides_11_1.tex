\documentclass{beamer}

\usepackage{../../../latex_style/beamerthemeExecushares}
\usepackage{../../../latex_style/notations}

\title{Video 11.1: Critical points, global and local extrema}
\subtitle{Optimization and Computational Linear Algebra for Data Science}
\author{LÃ©o Miolane}
\date{}

\setcounter{showSlideNumbers}{1}

\begin{document}
\setcounter{showProgressBar}{0}
\setcounter{showSlideNumbers}{0}

\frame{\titlepage}
\setcounter{framenumber}{0}
%\setcounter{showProgressBar}{1}
\setcounter{showSlideNumbers}{1}




\begin{frame}[t]{Definitions}
	\grid

	\vspace{-0.2cm}
	\begin{definition}
		Let $f: \R^n \to \R$ be a differentiable function.
		We say that $x \in \R^n$ is
		\begin{itemize}
			\item a critical point of $f$ if \ \ $\nabla f(x) = 0$,
			\item a \emph{global} minimizer of $f$ if for all $x' \in \R^n$, \ $f(x) \leq f(x')$,
			\item a \emph{local} minimizer of $f$ if there exists $\delta > 0$ such that we have $f(x) \leq f(x')$ for all $x'$ verifying $\|x-x'\| \leq \delta$.
		\end{itemize}

	\end{definition}

\end{frame}

\begin{frame}[t]{Local extrema are critical points}
	\grid

	\vspace{-0.2cm}
	\begin{block}{Proposition}
	\vspace{-0.2cm}
		$$
		x \ \text{is a local minimizer of} \ f \ \implies \nabla f(x) = 0.
		$$
	\end{block}

	\begin{block}{Proposition}
		Assume that $f$ is convex. Then
		$$
		\nabla f(x) = 0 \ \Longleftrightarrow \ x \ \text{is a global minimizer of} \ f.
		$$
	\end{block}

\end{frame}

\begin{frame}[t]{Looking at the Hessian}
	\grid

	\vspace{-0.2cm}
	\begin{block}{Proposition}
	Let $f:\R^n \to \R$ be a twice differentiable function. Let $x \in \R^n$ be a critical point of $f$, i.e.\ $\nabla f(x) = 0$. 

	Then, if $H_f(x)$ is positive definite (that is, if all the eigenvalues of $H_f(x)$ are strictly positive), then $x$ is a local minimizer of $f$.
	\end{block}

\end{frame}

\begin{frame}[t]{Looking at the Hessian}
	\grid

	\vspace{-0.2cm}
	\begin{block}{Proposition}
	Let $f:\R^n \to \R$ be a twice differentiable function. Let $x \in \R^n$ be a critical point of $f$, i.e.\ $\nabla f(x) = 0$. 

	Then, if $H_f(x)$ is negative definite (that is, if all the eigenvalues of $H_f(x)$ are strictly negative), then $x$ is a local maximizer of $f$.
	\end{block}

\end{frame}

\begin{frame}[t]{Saddle points}
	\grid

	\vspace{-0.2cm}
	\begin{block}{Proposition}
	Let $f:\R^n \to \R$ be a twice differentiable function. Let $x \in \R^n$ be a critical point of $f$, i.e.\ $\nabla f(x) = 0$. 

	Then, if $H_f(x)$ admits strictly positive eigenvalues and strictly negative eigenvalues, then $x$ is neither a local maximum nor a local minimum. We call $x$ a saddle point.
	\end{block}

\end{frame}

\begin{frame}[t]{Saddle points}
	\grid

	\begin{center}
		\includegraphics[width=10cm]{../figures/saddle-crop.pdf}
	\end{center}

\end{frame}
\begin{frame}[t]{Example}
	\grid

	Study the critical points of $f(x,y)=x^2 + xy^2 - x+1$.
\pause
\end{frame}

\end{frame}


\end{document}
